{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters\n",
    "Now that we have a model and data, let's use this notebook to to *train*, *validate* and *test* our model by optimizing its parameters on our data. \n",
    "\n",
    "Training is an iterative process; In each iteration, the model makes a guess about the output, calculates the error based on its guess (loss), and optimizes the parameters using gradient descent based on the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "*Hyperparameters* are adjustable parameters that let you control the model optimization process. They are different from the model parameters, which are learned during training.\n",
    "\n",
    "In this example, we will define the following hyperparameters:\n",
    "- `Number of Epochs`: The number of times to iterate over the training dataset.\n",
    "- `Batch Size`: The number of data samples propagated through the network before the parameters are updated.\n",
    "- `Learning Rate`: How much to update the model parameters at each batch based on the loss gradient. A small learning rate means the model learns slowly, while a large learning rate means it learns quickly but may overshoot the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "Once our hyperparameters are set, we can train our model with an optimization loop. Each iteration of the loop is called an *epoch*. Each epochs consists of 2 main parts:\n",
    "1. **Training**: The model is trained on the training dataset.\n",
    "2. **Validation / Testing**: Iterate over the validation/test dataset to check if model performance is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader : DataLoader\n",
    "        The data loader for training data.\n",
    "    model : nn.Module\n",
    "        The neural network model to train.\n",
    "    loss_fn : nn.Module\n",
    "        The loss function to use.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    '''\n",
    "    size = len(dataloader.dataset)          # total number of training samples\n",
    "\n",
    "    # Set the model to training mode (enables dropout, batch norm, etc.)\n",
    "    # unncessary for this simple model but good practice\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # clear previous gradients\n",
    "        loss.backward()         # compute gradients\n",
    "        optimizer.step()        # update weights\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            # Print loss every 100 batches\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader : DataLoader\n",
    "        The data loader for test data.\n",
    "    model : nn.Module\n",
    "        The neural network model to test.\n",
    "    loss_fn : nn.Module\n",
    "        The loss function to use.\n",
    "    '''\n",
    "    size = len(dataloader.dataset)          # total number of test samples\n",
    "    num_batches = len(dataloader)          # total number of batches\n",
    "\n",
    "    # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    # unncessary for this simple model but good practice\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    '''\n",
    "    Evaluating the model with torch.no_grad() ensures that gradients are not computed to reduce unnecessary memory usage and speed up computations.\n",
    "    '''\n",
    "    with torch.no_grad():                   # no need to track gradients during evaluation\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()   # accumulate loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # count correct predictions\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304772  [   64/60000]\n",
      "loss: 2.287111  [ 6464/60000]\n",
      "loss: 2.266696  [12864/60000]\n",
      "loss: 2.268206  [19264/60000]\n",
      "loss: 2.252316  [25664/60000]\n",
      "loss: 2.220776  [32064/60000]\n",
      "loss: 2.235157  [38464/60000]\n",
      "loss: 2.203969  [44864/60000]\n",
      "loss: 2.193308  [51264/60000]\n",
      "loss: 2.159923  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.157460 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.163246  [   64/60000]\n",
      "loss: 2.151571  [ 6464/60000]\n",
      "loss: 2.092443  [12864/60000]\n",
      "loss: 2.116298  [19264/60000]\n",
      "loss: 2.065917  [25664/60000]\n",
      "loss: 1.997988  [32064/60000]\n",
      "loss: 2.029382  [38464/60000]\n",
      "loss: 1.955623  [44864/60000]\n",
      "loss: 1.952384  [51264/60000]\n",
      "loss: 1.879130  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 1.884183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.909723  [   64/60000]\n",
      "loss: 1.883273  [ 6464/60000]\n",
      "loss: 1.765719  [12864/60000]\n",
      "loss: 1.814987  [19264/60000]\n",
      "loss: 1.701680  [25664/60000]\n",
      "loss: 1.646752  [32064/60000]\n",
      "loss: 1.667511  [38464/60000]\n",
      "loss: 1.581845  [44864/60000]\n",
      "loss: 1.599991  [51264/60000]\n",
      "loss: 1.491257  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.520493 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.579346  [   64/60000]\n",
      "loss: 1.552038  [ 6464/60000]\n",
      "loss: 1.399706  [12864/60000]\n",
      "loss: 1.480227  [19264/60000]\n",
      "loss: 1.360304  [25664/60000]\n",
      "loss: 1.346028  [32064/60000]\n",
      "loss: 1.358063  [38464/60000]\n",
      "loss: 1.299980  [44864/60000]\n",
      "loss: 1.328476  [51264/60000]\n",
      "loss: 1.219916  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.259242 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.328955  [   64/60000]\n",
      "loss: 1.317762  [ 6464/60000]\n",
      "loss: 1.147285  [12864/60000]\n",
      "loss: 1.261551  [19264/60000]\n",
      "loss: 1.134367  [25664/60000]\n",
      "loss: 1.147485  [32064/60000]\n",
      "loss: 1.165550  [38464/60000]\n",
      "loss: 1.122414  [44864/60000]\n",
      "loss: 1.156033  [51264/60000]\n",
      "loss: 1.059512  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.093729 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.159060  [   64/60000]\n",
      "loss: 1.167104  [ 6464/60000]\n",
      "loss: 0.978709  [12864/60000]\n",
      "loss: 1.122032  [19264/60000]\n",
      "loss: 0.991710  [25664/60000]\n",
      "loss: 1.011453  [32064/60000]\n",
      "loss: 1.044353  [38464/60000]\n",
      "loss: 1.007324  [44864/60000]\n",
      "loss: 1.041454  [51264/60000]\n",
      "loss: 0.958695  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.985171 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.038227  [   64/60000]\n",
      "loss: 1.067591  [ 6464/60000]\n",
      "loss: 0.861613  [12864/60000]\n",
      "loss: 1.028072  [19264/60000]\n",
      "loss: 0.900293  [25664/60000]\n",
      "loss: 0.914503  [32064/60000]\n",
      "loss: 0.964606  [38464/60000]\n",
      "loss: 0.930743  [44864/60000]\n",
      "loss: 0.961059  [51264/60000]\n",
      "loss: 0.891154  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.910504 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.948089  [   64/60000]\n",
      "loss: 0.997698  [ 6464/60000]\n",
      "loss: 0.777113  [12864/60000]\n",
      "loss: 0.961614  [19264/60000]\n",
      "loss: 0.838586  [25664/60000]\n",
      "loss: 0.843245  [32064/60000]\n",
      "loss: 0.908798  [38464/60000]\n",
      "loss: 0.878825  [44864/60000]\n",
      "loss: 0.902620  [51264/60000]\n",
      "loss: 0.842907  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.856773 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.878688  [   64/60000]\n",
      "loss: 0.945347  [ 6464/60000]\n",
      "loss: 0.714103  [12864/60000]\n",
      "loss: 0.912516  [19264/60000]\n",
      "loss: 0.794708  [25664/60000]\n",
      "loss: 0.789978  [32064/60000]\n",
      "loss: 0.866860  [38464/60000]\n",
      "loss: 0.842435  [44864/60000]\n",
      "loss: 0.858818  [51264/60000]\n",
      "loss: 0.806522  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.816338 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.823764  [   64/60000]\n",
      "loss: 0.904009  [ 6464/60000]\n",
      "loss: 0.665610  [12864/60000]\n",
      "loss: 0.875019  [19264/60000]\n",
      "loss: 0.761631  [25664/60000]\n",
      "loss: 0.749436  [32064/60000]\n",
      "loss: 0.833422  [38464/60000]\n",
      "loss: 0.815594  [44864/60000]\n",
      "loss: 0.824688  [51264/60000]\n",
      "loss: 0.777594  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.784491 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # loss function for multi-class classification\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # SGD optimizer\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
